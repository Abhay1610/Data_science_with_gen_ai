{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**FEATURE ENGINEERING (ML) - ASSIGNMENT**\n",
        "##**Question**"
      ],
      "metadata": {
        "id": "T-_EC3Zmqrm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is a parameter?"
      ],
      "metadata": {
        "id": "t6SZuOfeuAYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: In the context of a machine learning model, a **parameter** is a configuration variable internal to the model whose value can be estimated from data. These values are learned during the training process and define the specific transformation the model performs on input data to produce predictions. Examples include the weights and biases in a neural network or the coefficients in a linear regression model."
      ],
      "metadata": {
        "id": "yon899b6uAVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "-PERMfMOuATE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Correlation** is a statistical measure that expresses the extent to which two variables are linearly related (i.e., they change together at a constant rate). It indicates the strength and direction of a linear relationship between two variables. The correlation coefficient ranges from -1 to +1, where +1 signifies a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 no linear relationship.\n",
        "\n",
        "**Negative correlation** means that as one variable increases, the other variable tends to decrease. In other words, the variables move in opposite directions. For example, there might be a negative correlation between the number of hours spent watching TV and a student's exam scores; as TV hours increase, exam scores might decrease."
      ],
      "metadata": {
        "id": "Can_KDSNuAQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "C_bDvYBwuAIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Machine Learning** is a subset of Artificial Intelligence that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention. The main components typically include: Data (the raw information used for learning), Model (the algorithm or structure that learns from data), Objective Function/Loss Function (measures how well the model is performing), and an Optimization Algorithm (adjusts model parameters to minimize the loss)."
      ],
      "metadata": {
        "id": "YwkPYnxbuAF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "R5Ymwy47uADQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **loss value**, or **cost function**, quantifies the error between the predicted output of a model and the actual target values. A lower loss value generally indicates a better-performing model, as it means the model's predictions are closer to the true values. During training, the goal is to minimize this loss, guiding the optimization process to find optimal model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "ksTYcWvpuAAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "Pasm43Dht_-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "1. **Continuous variables** are numerical variables\n",
        "that can take any value within a given range, often involving decimals. Examples include height, weight, temperature, or age.\n",
        "2. **Categorical variables** are variables that represent distinct categories or groups. They can be nominal (no inherent order, e.g., colors) or ordinal (have an order, e.g., educational levels: high school, college, graduate)."
      ],
      "metadata": {
        "id": "mwBn-nzMt_7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "tkQABgABt_44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Categorical variables** need to be converted into numerical representations for most machine learning algorithms. Common techniques include:\n",
        "\n",
        "\n",
        "*   **One-Hot Encoding**: Creates new binary columns for each category, indicating presence (1) or absence (0).\n",
        "\n",
        "*   **Label Encoding**: Assigns a unique integer to each category. This is suitable for ordinal categories but can imply a false order for nominal ones.\n",
        "\n",
        "*   **Target Encoding**: Replaces a category with the mean of the target variable for that category."
      ],
      "metadata": {
        "id": "xUPxutbht_2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "Tjlw4-wut_zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "1.   **Training a dataset** involves using a portion of the data to fit the machine learning model. The model learns patterns and relationships from this data to adjust its internal parameters.\n",
        "2.   **Testing a dataset** involves evaluating the trained model's performance on a separate, unseen portion of the data. This helps assess how well the model generalizes to new data and avoids overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "haZjaBD6t_uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "OKBlWqYJt_jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **sklearn.preprocessing** is a module within the scikit-learn library in Python that provides a wide range of tools for data preprocessing. This includes functions for scaling features (e.g., StandardScaler, MinMaxScaler), encoding categorical variables (e.g., OneHotEncoder, LabelEncoder), and other transformations necessary to prepare data for machine learning algorithms."
      ],
      "metadata": {
        "id": "zqIHIY5lxFt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What is a Test set?"
      ],
      "metadata": {
        "id": "PhRYemesxFrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: A **test set** is a subset of the original dataset that is held out from the training process. Its purpose is to provide an unbiased evaluation of the final model's performance. Since the model has never seen this data during training, the metrics obtained on the test set are a good indicator of how well the model will generalize to real-world, unseen data."
      ],
      "metadata": {
        "id": "r3QPsiwLxFpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "AKBsuy8UxFmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: We typically use the train_test_split function from sklearn.model_selection to split data."
      ],
      "metadata": {
        "id": "qLpNNMYexFkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy DataFrame for demonstration\n",
        "data = pd.DataFrame(np.random.rand(100, 5), columns=[f'feature_{i}' for i in range(5)])\n",
        "target = pd.Series(np.random.randint(0, 2, 100))\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGqtnmfIw3cY",
        "outputId": "dc5269a5-f43c-48d8-b98f-7a26a2813854"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (80, 5)\n",
            "X_test shape: (20, 5)\n",
            "y_train shape: (80,)\n",
            "y_test shape: (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common approach to a **machine learning problem** involves several steps:\n",
        "\n",
        "1. **Problem Definition**: Clearly understand the objective and success metrics.\n",
        "\n",
        "2. **Data Collection**: Gather relevant data.\n",
        "\n",
        "3. **Exploratory Data Analysis (EDA)**: Understand data characteristics, distributions, and relationships.\n",
        "\n",
        "4. **Data Preprocessing**: Handle missing values, outliers, and prepare data for modeling (e.g., encoding, scaling).\n",
        "\n",
        "5. **Model Selection**: Choose appropriate algorithms based on the problem type.\n",
        "\n",
        "6. **Model Training**: Fit the model to the training data.\n",
        "\n",
        "7. **Model Evaluation**: Assess performance using metrics on the test set.\n",
        "\n",
        "8. **Hyperparameter Tuning**: Optimize model parameters.\n",
        "\n",
        "9. **Deployment**: Put the model into production."
      ],
      "metadata": {
        "id": "rv3LeLB-xFfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "gS3_ni24xFb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Performing **Exploratory Data Analysis (EDA)** before fitting a model is crucial because it provides valuable insights into the dataset's structure, distributions, and potential issues. EDA helps in:\n",
        "\n",
        "\n",
        "*   Identifying missing values, outliers, or errors.\n",
        "*   Understanding relationships between variables (e.g., correlations).\n",
        "*   Guiding feature engineering and selection.\n",
        "*   Informing decisions about preprocessing techniques.\n",
        "*   Uncovering patterns that might influence model choice.\n",
        "*   It allows us to prepare the data effectively and choose a suitable model, ultimately leading to better model performance."
      ],
      "metadata": {
        "id": "sCV9rDA4xFZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. What is correlation?"
      ],
      "metadata": {
        "id": "3TZadmYrxFXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Correlation** is a statistical measure that expresses the extent to which two variables are linearly related (i.e., they change together at a constant rate). It indicates the strength and direction of a linear relationship between two variables. The correlation coefficient ranges from -1 to +1, where +1 signifies a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 no linear relationship."
      ],
      "metadata": {
        "id": "_rrLPotZxFUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. What is negative correlation?"
      ],
      "metadata": {
        "id": "hw1B_SOaxFQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Negative correlation** means that as one variable increases, the other variable tends to decrease. In other words, the variables move in opposite directions. For example, there might be a negative correlation between the number of hours spent watching TV and a student's exam scores; as TV hours increase, exam scores might decrease."
      ],
      "metadata": {
        "id": "GOLXXAIMxFOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "uYz3sJo5xFLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: We can find the correlation matrix of a DataFrame using the .corr() method in pandas."
      ],
      "metadata": {
        "id": "tDsGbIWIxFI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy DataFrame\n",
        "data = pd.DataFrame(np.random.rand(10, 3), columns=['A', 'B', 'C'])\n",
        "data['D'] = data['A'] * 2 + np.random.rand(10) * 0.1 # Introduce some correlation\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHhwh2Q70H-Z",
        "outputId": "3fcf3aa2-3e85-4df8-9671-a1f854d93184"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C         D\n",
            "A  1.000000 -0.574066 -0.459828  0.998139\n",
            "B -0.574066  1.000000  0.469171 -0.583582\n",
            "C -0.459828  0.469171  1.000000 -0.467196\n",
            "D  0.998139 -0.583582 -0.467196  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "3TyVw051xFFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Causation means that one event is the direct result of another event; a change in one variable directly leads to a change in another. Correlation, on the other hand, only indicates that two variables tend to change together, but it doesn't imply that one causes the other.\n",
        "\n",
        "Example:\n",
        "\n",
        "1. Correlation: Ice cream sales and drowning incidents often show a positive correlation. Both tend to increase in summer.\n",
        "\n",
        "2. Causation: The cause is hot weather, which leads to both more ice cream consumption and more swimming (and thus, unfortunately, more drownings). Ice cream sales do not cause drownings, nor do drownings cause ice cream sales."
      ],
      "metadata": {
        "id": "TY1MpZM10DWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "qQ6ttTgj0DSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: An **Optimizer** is an algorithm used to adjust the parameters of a machine learning model during training to minimize the loss function. It determines how the model's weights and biases are updated in response to the calculated loss.\n",
        "\n",
        "Different types of optimizers include:\n",
        "\n",
        "1. **Gradient Descent (GD)**: Updates parameters by taking steps proportional to the negative of the gradient of the loss function. It's computationally expensive for large datasets as it uses the entire dataset for each update.\n",
        "\n",
        " *Example*: Imagine climbing down a hill (loss function) by always taking a step in the steepest downward direction.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD)**: Updates parameters using the gradient of the loss function calculated on a single randomly chosen training example at a time. This makes updates faster and can escape local minima but introduces more noise.\n",
        "\n",
        " *Example*: Taking a step down the hill based on just one small observation of the slope.\n",
        "\n",
        "3. **Mini-Batch Gradient Descent**: A compromise between GD and SGD. It updates parameters using gradients calculated on small batches of training examples. This balances computational efficiency with stability.\n",
        "\n",
        " *Example*: Taking a step down the hill based on the average slope observed from a small group of people.\n",
        "\n",
        "4. **Adam (Adaptive Moment Estimation)**: An adaptive learning rate optimization algorithm that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. It's often a good default choice due to its efficiency and effectiveness.\n",
        "\n",
        " *Example*: More sophisticated descent, where each step considers not just the current slope but also the history of how steep the path has been in different directions, allowing for faster and more precise descent."
      ],
      "metadata": {
        "id": "zvUaRfbY0DO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. What is sklearn.linear_model?"
      ],
      "metadata": {
        "id": "yJZgt6M60DLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **sklearn.linear_model** is a module within the scikit-learn library that provides a collection of linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable. Examples include LinearRegression, LogisticRegression, Ridge, Lasso, and ElasticNet."
      ],
      "metadata": {
        "id": "jn-Os9p-0DH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "KXIx3nt_0DEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **model.fit()** method is used to train a machine learning model on the provided training data. During this process, the model learns the underlying patterns and relationships in the data by adjusting its internal parameters (e.g., weights, biases) to minimize its loss function.\n",
        "\n",
        "The essential arguments that must be given are:\n",
        "\n",
        "1) X (features): The training input samples, typically a 2D array or DataFrame where rows are samples and columns are features.\n",
        "\n",
        "2) y (target): The target values (labels for classification, continuous values for regression), typically a 1D array or Series."
      ],
      "metadata": {
        "id": "6zsmhEVA0DBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Create some dummy data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Model has been fitted.\")\n",
        "print(f\"Coefficient (slope): {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pDc9av32JGA",
        "outputId": "1c3fc4c4-5737-44a8-bfb0-7bee8fa8b3e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been fitted.\n",
            "Coefficient (slope): 0.60\n",
            "Intercept: 2.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "xBJ5xETP0C-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **model.predict()** method is used to generate predictions using a trained machine learning model. Once a model has been fit() to training data, you can use predict() to estimate target values for new, unseen input data.\n",
        "\n",
        "The essential argument that must be given is:\n",
        "*   X (features): The input samples for which you want to make predictions, typically a 2D array or DataFrame with the same number of features as the training data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MEjWcEHR0C6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Create some dummy data and fit a model (as above)\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([2, 4, 5, 4, 5])\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = np.array([[6], [7], [8]])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(f\"New data points: {X_new.flatten()}\")\n",
        "print(f\"Predicted values: {predictions.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJdAYmxD235g",
        "outputId": "01f933e0-bd9e-4742-c492-a903d4583c72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New data points: [6 7 8]\n",
            "Predicted values: [5.8 6.4 7. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "rj7hqhhW0C3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "1. **Continuous variables** are numerical variables\n",
        "that can take any value within a given range, often involving decimals. Examples include height, weight, temperature, or age.\n",
        "2. **Categorical variables** are variables that represent distinct categories or groups. They can be nominal (no inherent order, e.g., colors) or ordinal (have an order, e.g., educational levels: high school, college, graduate)."
      ],
      "metadata": {
        "id": "ZR8WZf2a0C0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "egrZfbv60Cxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Feature scaling** is a data preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the distance calculations or gradient descent optimizations.\n",
        "\n",
        "It helps in Machine Learning by:\n",
        "\n",
        "1. Preventing dominance: Features with larger ranges don't disproportionately influence the model.\n",
        "\n",
        "2. Faster convergence: Speeds up gradient descent algorithms by avoiding zigzagging in the optimization landscape.\n",
        "\n",
        "3. Improved performance for distance-based algorithms: Essential for algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means clustering, where distances between data points are crucial."
      ],
      "metadata": {
        "id": "OT-Fxbd10CtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "QrTmpYrs3-Mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: We typically use StandardScaler or MinMaxScaler from sklearn.preprocessing."
      ],
      "metadata": {
        "id": "fMBxqqHV3-Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create dummy data with different scales\n",
        "data = pd.DataFrame({\n",
        "    'feature_A': np.random.rand(10) * 1000,\n",
        "    'feature_B': np.random.rand(10) * 10\n",
        "})\n",
        "\n",
        "print(\"Original Data:\\n\", data)\n",
        "\n",
        "# Standard Scaling\n",
        "scaler_standard = StandardScaler()\n",
        "scaled_data_standard = scaler_standard.fit_transform(data)\n",
        "print(\"\\nStandard Scaled Data:\\n\", pd.DataFrame(scaled_data_standard, columns=data.columns))\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "scaled_data_minmax = scaler_minmax.fit_transform(data)\n",
        "print(\"\\nMin-Max Scaled Data:\\n\", pd.DataFrame(scaled_data_minmax, columns=data.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAQgVSp4Ox7",
        "outputId": "a9a4eef0-c49c-49c5-c5c1-8e8cf777901b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "     feature_A  feature_B\n",
            "0  906.316695   3.122340\n",
            "1  159.637038   2.754682\n",
            "2  225.740649   7.450629\n",
            "3  863.907117   3.062327\n",
            "4  829.258349   3.877637\n",
            "5  964.273257   4.582300\n",
            "6  624.046140   9.920757\n",
            "7  889.373884   0.763540\n",
            "8   28.377275   2.413477\n",
            "9  177.281560   1.035362\n",
            "\n",
            "Standard Scaled Data:\n",
            "    feature_A  feature_B\n",
            "0   0.955862  -0.288797\n",
            "1  -1.146442  -0.425630\n",
            "2  -0.960325   1.322093\n",
            "3   0.836457  -0.311132\n",
            "4   0.738902  -0.007692\n",
            "5   1.119041   0.254567\n",
            "6   0.161119   2.241418\n",
            "7   0.908159  -1.166687\n",
            "8  -1.516009  -0.552619\n",
            "9  -1.096763  -1.065522\n",
            "\n",
            "Min-Max Scaled Data:\n",
            "    feature_A  feature_B\n",
            "0   0.938074   0.257589\n",
            "1   0.140250   0.217440\n",
            "2   0.210882   0.730253\n",
            "3   0.892759   0.251035\n",
            "4   0.855737   0.340070\n",
            "5   1.000000   0.417022\n",
            "6   0.636469   1.000000\n",
            "7   0.919970   0.000000\n",
            "8   0.000000   0.180179\n",
            "9   0.159103   0.029684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "WN3ApQ3637Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The **sklearn.preprocessing** is a module within the scikit-learn library in Python that provides a wide range of tools for data preprocessing. This includes functions for scaling features (e.g., StandardScaler, MinMaxScaler), encoding categorical variables (e.g., OneHotEncoder, LabelEncoder), and other transformations necessary to prepare data for machine learning algorithms."
      ],
      "metadata": {
        "id": "0o_JWsZS37Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "IGK8ZJHh4xo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The most common way to **split data** in Python for machine learning model fitting is by using the train_test_split function from sklearn.model_selection. This function randomly partitions a dataset into distinct training and testing subsets, ensuring that the model is evaluated on data it has not seen during the training phase."
      ],
      "metadata": {
        "id": "lXSb20MG5Sku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. Explain data encoding?"
      ],
      "metadata": {
        "id": "wrtLLYR05nTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Data encoding** is the process of converting categorical data (which represents distinct categories or labels) into a numerical format that machine learning algorithms can understand and process. Most algorithms require numerical input, so categorical variables, such as \"colors\" (e.g., Red, Blue, Green) or \"cities\" (e.g., New York, London), must be transformed. Common encoding techniques include One-Hot Encoding (creating binary columns for each category) and Label Encoding (assigning a unique integer to each category). The choice of encoding depends on the nature of the categorical variable (nominal vs. ordinal) and the specific algorithm being used."
      ],
      "metadata": {
        "id": "5wAPbe0w5sdD"
      }
    }
  ]
}