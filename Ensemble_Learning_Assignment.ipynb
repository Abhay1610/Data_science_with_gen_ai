{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | **Assignment**"
      ],
      "metadata": {
        "id": "jzf9yodu8vDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it\n"
      ],
      "metadata": {
        "id": "F9U9ahX6tNfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Ensemble Learning is a machine learning technique where multiple models—often called base learners—are combined to produce a final model that is more accurate and robust than any individual model.\n",
        "\n",
        "The main idea is that while each base model may be weak or make errors in different ways, combining their predictions—through averaging, voting, or more complex methods—can reduce overall errors like bias and variance. In essence, collective decisions from diverse models often outperform any single model.\n",
        "\n",
        "Ensemble strategies commonly include:\n",
        "\n",
        "* Bagging or Bootstrap Aggregating\n",
        "* Boosting\n",
        "* Stacking or Stacked Generalization"
      ],
      "metadata": {
        "id": "lzPcXpLStNiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: What is the difference between Bagging and Boosting?\n"
      ],
      "metadata": {
        "id": "q_OW9hK_tNlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Difference Between Bagging and Boosting\n",
        "\n",
        "1. *Bagging (Bootstrap Aggregating)*\n",
        "\n",
        "   * Trains multiple independent base learners in parallel on different random subsets of the data, created via bootstrap sampling (sampling with replacement).\n",
        "   \n",
        "   * Aims to reduce variance and help prevent overfitting, especially effective for high-variance models like decision trees.\n",
        "   \n",
        "   * Aggregates predictions through majority voting (classification) or averaging (regression).\n",
        "   \n",
        "   * Example: Random Forest, which adds extra randomness by selecting different features at each split to further lower correlation among trees.\n",
        "\n",
        "\n",
        "2. *Boosting*\n",
        "\n",
        "   * Builds models sequentially—each new learner focuses on correcting errors made by previous ones.\n",
        "\n",
        "   * Targets reducing bias by combining many weak learners into a strong predictive model.\n",
        "\n",
        "   * Predictions are combined using weighted sums or weighted voting, with  higher weights for better-performing learners.\n",
        "\n",
        "   * Examples include AdaBoost, Gradient Boosting, XGBoost, and CatBoost."
      ],
      "metadata": {
        "id": "R208rNH1tNn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n"
      ],
      "metadata": {
        "id": "jsFCcmN8tNqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: **Bootstrap sampling** is a statistical resampling technique in which you repeatedly draw samples with replacement from an original dataset. Each drawn sample (called a bootstrap sample) has the same size as the original dataset, but due to replacement, some observations may appear multiple times while others may be omitted.\n",
        "\n",
        "*Role of Bootstrap Sampling in Bagging & Random Forest*\n",
        "\n",
        "1. Creating Diverse Training Subsets:\n",
        "Bagging (Bootstrap Aggregating) uses bootstrap sampling to generate multiple distinct subsets of the training data. Each subset trains its own independent base learner.\n",
        "\n",
        "2. Reducing Variance & Overfitting:\n",
        "High-variance models (like deep decision trees) are prone to overfitting. When many such models are trained on different bootstrap samples and aggregated (via average or majority vote), the overall variance drops, and overfitting is mitigated.\n",
        "\n",
        "3. Usage in Random Forests:\n",
        "Random Forest applies bootstrap sampling to train each decision tree on a unique sample from the data. Combined with random feature selection, this approach decorrelates the trees and further reduces variance without increasing bias."
      ],
      "metadata": {
        "id": "f92gXhQTtNsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n"
      ],
      "metadata": {
        "id": "B-N4qvaXtNvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: **Out-of-Bag (OOB) samples** are the data points not included in the bootstrap sample used to train a particular base learner (like a decision tree) in methods such as Bagging or Random Forest. When sampling with replacement, only about 63 % of the original dataset is included in each bootstrap sample—leaving roughly 37 % as OOB for that tree.\n",
        "\n",
        "*How Are OOB Samples Used for Evaluation?*\n",
        "\n",
        "1. Training and OOB Division:\n",
        "Each tree is trained on its unique bootstrap sample, and the omitted samples become that tree’s OOB samples.\n",
        "\n",
        "2. Prediction on OOB Samples:\n",
        "After training, each tree predicts the outcomes for its own OOB samples. Since these samples were never used in training that tree, they serve as unbiased “unseen” data.\n",
        "\n",
        "3. Aggregation Across Trees:\n",
        "Each data point in the dataset is likely OOB for several trees. For classification, predictions are combined via majority vote; for regression, averaged.\n",
        "\n",
        "4. Compute OOB Score / Error:\n",
        "The aggregated OOB predictions are compared with actual values to compute:\n",
        "   * OOB Score (e.g., accuracy for classification)\n",
        "   * OOB Error (e.g., misclassification rate or MSE)"
      ],
      "metadata": {
        "id": "S7W54S87tNyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n"
      ],
      "metadata": {
        "id": "6gKKZfkjtN0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**:\n",
        "1. Single Decision Tree -\n",
        "\n",
        "   * Calculation Method : Feature importance is computed based on how much each feature reduces impurity (like Gini impurity or entropy) across all splits using that feature. The score for each feature is the total (and then normalized) reduction in impurity attributed to that feature.\n",
        "\n",
        "   * Characteristics\n",
        "\n",
        "     * Easy to interpret—feature contributions can be traced through the tree.\n",
        "\n",
        "     * However, because the tree can overfit to the data, importance scores may become biased or overly sensitive to noise.\n",
        "\n",
        "2. Random Forest -\n",
        "   * How It's Derived:\n",
        "      * Importance is first calculated for each feature in every tree (via impurity reduction methods like Gini importance).\n",
        "\n",
        "      * These values are then averaged across all trees in the forest.\n",
        "\n",
        "      * Finally, the averaged scores are normalized to sum to 1.\n",
        "\n",
        "   * Why It’s More Reliable:\n",
        "\n",
        "      * Each tree is trained on different subsets of data and features, so averaging importance scores yields more stable and robust estimates. A feature overly favored in one tree is less likely to dominate overall.\n",
        "\n",
        "   * Limitations & Caveats\n",
        "\n",
        "     * Importance scores from impurity-based methods may be biased toward continuous or high-cardinality features.\n",
        "\n",
        "     * Correlated features may split credit unevenly, making individual importance scores less clear."
      ],
      "metadata": {
        "id": "5-UBhZAltN30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Write a Python program to:\n",
        "####● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "####● Train a Random Forest Classifier\n",
        "\n",
        "####● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "hoELL6e6tN6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Data\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Split the data into X and y\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importances.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKnwKJZgt61Q",
        "outputId": "2b3faf0a-cbd3-4b47-ad6c-5d5e21559382"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "22       worst perimeter    0.141142\n",
            "27  worst concave points    0.125184\n",
            "23            worst area    0.115155\n",
            "20          worst radius    0.089507\n",
            "7    mean concave points    0.081823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Write a Python program to:\n",
        "###● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "###● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "6Zjjirz6tN88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_acc)\n",
        "print(\"Accuracy of Bagging Classifier (with Decision Trees):\", bagging_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FowRlldBv0bq",
        "outputId": "680e24c8-381e-42f5-bc3c-c425d342cbd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 0.9333333333333333\n",
            "Accuracy of Bagging Classifier (with Decision Trees): 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Write a Python program to:\n",
        "####● Train a Random Forest Classifier\n",
        "####● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "####● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "OUWjB4EitN_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R81K9cOEw0cL",
        "outputId": "9c70a894-1daa-4bcf-b41e-347e44a894e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Write a Python program to:\n",
        "####● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "####● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "KYCwv5jCtOCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ6HcnRxyMOl",
        "outputId": "db6a756e-bf5a-4a21-efc7-52e69108e289"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2572988359842641\n",
            "Mean Squared Error (Random Forest Regressor): 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "####● Choose between Bagging or Boosting\n",
        "####● Handle overfitting\n",
        "####● Select base models\n",
        "####● Evaluate performance using cross-validation\n",
        "####● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "####(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "_O7a_iAUtOFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step:\n",
        "\n",
        "1. Bagging vs Boosting\n",
        "\n",
        "   * Use Bagging (e.g., Random Forest) to reduce variance & overfitting.\n",
        "\n",
        "   * Use Boosting (e.g., Gradient Boosting) to reduce bias by focusing on hard cases.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "   * Limit max_depth, use min_samples_leaf, early stopping (Boosting).\n",
        "\n",
        "   * Monitor CV gap between train & test.\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "   * Bagging → full decision trees.\n",
        "\n",
        "   * Boosting → shallow trees (stumps).\n",
        "\n",
        "4. Evaluate with Cross-Validation\n",
        "\n",
        "   * Use StratifiedKFold for imbalanced default prediction.\n",
        "\n",
        "   * Metrics: ROC-AUC and Average Precision (PR curve).\n",
        "\n",
        "5. Why Ensembles Help\n",
        "\n",
        "   * Combine weak models → better accuracy, lower variance/bias.\n",
        "\n",
        "   * Improves credit risk prediction → fewer missed defaults, more robust loan approvals."
      ],
      "metadata": {
        "id": "bSTkly_0tOIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Simulated imbalanced loan default dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "\n",
        "print(\"RF CV ROC-AUC:\", cross_val_score(rf, X_train, y_train, cv=cv, scoring=\"roc_auc\").mean())\n",
        "print(\"GB CV ROC-AUC:\", cross_val_score(gb, X_train, y_train, cv=cv, scoring=\"roc_auc\").mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNWOZqbXaWHI",
        "outputId": "65f5475c-dded-4215-9982-340450d6face"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF CV ROC-AUC: 0.9247606808169475\n",
            "GB CV ROC-AUC: 0.9221359208568722\n"
          ]
        }
      ]
    }
  ]
}